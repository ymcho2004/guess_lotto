import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler
import os

WINDOW_SIZE = 5      
HIDDEN_SIZE = 128    
LAYERS = 2           
LEARNING_RATE = 0.001
EPOCHS = 200         

# 1. ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤
class LottoDataset(Dataset):
    def __init__(self, data):
        self.x_data = []
        self.y_data = []
        
        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ë°ì´í„° ìë¥´ê¸°
        for i in range(len(data) - WINDOW_SIZE):
            x = data[i : i + WINDOW_SIZE] 
            y = data[i + WINDOW_SIZE]     
            
            self.x_data.append(x)
            self.y_data.append(y)
            
        self.x_data = torch.tensor(np.array(self.x_data), dtype=torch.float32)
        self.y_data = torch.tensor(np.array(self.y_data), dtype=torch.float32)

    def __len__(self):
        return len(self.x_data)

    def __getitem__(self, idx):
        return self.x_data[idx], self.y_data[idx]

class LottoLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LottoLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size) 

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        # LSTM í†µê³¼
        out, _ = self.lstm(x, (h0, c0))
        
        # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ê²°ê³¼ë§Œ ê°€ì ¸ì˜¤ê¸°
        out = self.fc(out[:, -1, :])
        return out

# --- 2. ë©”ì¸ ì‹¤í–‰ ì½”ë“œ ---
if __name__ == "__main__":
    # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    if not os.path.exists("lotto_history.csv"):
        print("âŒ ë°ì´í„° íŒŒì¼ì´ ì—†ì–´ìš”! get_data.py ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        exit()
        
    df = pd.read_csv("lotto_history.csv")
    
    # í•„ìš”í•œ ë²ˆí˜¸ë§Œ ê°€ì ¸ì˜¤ê¸°
    numbers = df[['drwtNo1', 'drwtNo2', 'drwtNo3', 'drwtNo4', 'drwtNo5', 'drwtNo6']].values
    
    scaler = MinMaxScaler()
    numbers_scaled = scaler.fit_transform(numbers)
    
    # ë°ì´í„° ë‚˜ëˆ„ê¸° (1~1000íšŒ: í•™ìŠµìš© / 1001~ë: ê²€ì¦ìš©)
    # ì£¼ì˜: WINDOW_SIZE ë§Œí¼ ë°ì´í„°ê°€ ë°€ë¦¬ë¯€ë¡œ ì¸ë±ìŠ¤ ê³„ì‚° í•„ìš”
    train_data = numbers_scaled[:1000]
    test_data = numbers_scaled[1000 - WINDOW_SIZE:] # 1001íšŒë¥¼ ë§ì¶”ë ¤ë©´ ì•ë°ì´í„°ê°€ í•„ìš”í•˜ë‹ˆê¹Œ ì¡°ê¸ˆ ê²¹ì¹˜ê²Œ ê°€ì ¸ì˜´
    
    train_dataset = LottoDataset(train_data)
    test_dataset = LottoDataset(test_data)
    
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False) # ì‹œê³„ì—´ì´ë¼ ì…”í”Œ ì•ˆ í•˜ëŠ” ê²Œ ë³´í†µ
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)
    
    # ëª¨ë¸ ìƒì„±
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ ì‚¬ìš©í•˜ëŠ” ì¥ì¹˜: {device}")
    
    model = LottoLSTM(input_size=6, hidden_size=HIDDEN_SIZE, num_layers=LAYERS, output_size=6).to(device)
    
    criterion = nn.MSELoss() # ì†ì‹¤í•¨ìˆ˜ (ì •ë‹µê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ ê³„ì‚°)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # --- í•™ìŠµ ì‹œì‘ ---
    print("ğŸ§  í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    model.train()
    for epoch in range(EPOCHS):
        total_loss = 0
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            
            optimizer.zero_grad()
            output = model(x)
            loss = criterion(output, y)
            
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            
        if (epoch+1) % 20 == 0:
            print(f"Epoch [{epoch+1}/{EPOCHS}], Loss: {total_loss/len(train_loader):.6f}")

    # --- ëª¨ë¸ ì €ì¥ ---
    torch.save(model.state_dict(), "lotto_lstm.pth")
    print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ! (lotto_lstm.pth)")
    
    # --- ê²€ì¦ (1001íšŒë¶€í„° ì˜ˆì¸¡ í•´ë³´ê¸°) ---
    print("\nğŸ” ê²€ì¦ ë°ì´í„°(1001íšŒ~) ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸")
    model.eval()
    with torch.no_grad():
        # ë”± í•˜ë‚˜ë§Œ ì˜ˆì‹œë¡œ í…ŒìŠ¤íŠ¸
        sample_x, sample_y = test_dataset[0] # 1001íšŒì°¨ ì˜ˆì¸¡ì„ ìœ„í•œ ì…ë ¥
        sample_x = sample_x.unsqueeze(0).to(device)
        
        prediction = model(sample_x)
        
        # ìŠ¤ì¼€ì¼ë§ ëœ ê±¸ ë‹¤ì‹œ ì›ë˜ ë¡œë˜ ë²ˆí˜¸ë¡œ ë³µêµ¬
        predicted_numbers = scaler.inverse_transform(prediction.cpu().numpy())
        real_numbers = scaler.inverse_transform(sample_y.unsqueeze(0).numpy())
        
        print(f"ì˜ˆì¸¡ê°’: {np.round(predicted_numbers).astype(int)}")
        print(f"ì •ë‹µê°’: {real_numbers.astype(int)}")